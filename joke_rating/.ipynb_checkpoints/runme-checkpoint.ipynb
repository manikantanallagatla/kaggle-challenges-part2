{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = pd.read_csv('data_1.csv', delimiter= \",\", encoding='utf-8', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv('train.csv', delimiter= \",\", encoding='utf-8', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_csv[3][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1 = train_csv[1][1:]\n",
    "train_x2 = train_csv[2][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "for i in range(1,1092060):\n",
    "#     print((train_x2[i]) )\n",
    "#     print( jokes.iloc[ int(train_x2[i]) ] )\n",
    "    row = np.concatenate(([int(train_x1[i])], np.transpose(np.array(jokes.iloc[ int(train_x2[i]) ]))) , axis= 0)\n",
    "#     print(row)\n",
    "    train_x.append(row)\n",
    "    \n",
    "# train_x\n",
    "# train_x1 = np.array(train_x)\n",
    "# train_x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_arr = np.array(train_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1092059, 3001)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                        2.75\n",
       "2          5.0939999999999985\n",
       "3                      -6.438\n",
       "4          4.4060000000000015\n",
       "5                       9.375\n",
       "6                      -1.781\n",
       "7                        6.25\n",
       "8                      -2.906\n",
       "9                       -5.75\n",
       "10                      1.656\n",
       "11                      1.656\n",
       "12                      0.688\n",
       "13         1.3119999999999998\n",
       "14         6.0310000000000015\n",
       "15                      9.781\n",
       "16                       8.75\n",
       "17                      0.156\n",
       "18                      3.562\n",
       "19                      2.938\n",
       "20                       3.75\n",
       "21                     -1.906\n",
       "22                      3.469\n",
       "23                        7.0\n",
       "24         4.6560000000000015\n",
       "25                      1.719\n",
       "26         6.2189999999999985\n",
       "27                      -8.25\n",
       "28                      5.188\n",
       "29                     -2.281\n",
       "30                      1.406\n",
       "                  ...        \n",
       "1092030                -0.125\n",
       "1092031                 5.438\n",
       "1092032                  5.25\n",
       "1092033                 -7.25\n",
       "1092034                 1.625\n",
       "1092035                -0.281\n",
       "1092036                -2.594\n",
       "1092037                -0.094\n",
       "1092038                 4.812\n",
       "1092039                -2.562\n",
       "1092040                 3.188\n",
       "1092041                 4.031\n",
       "1092042                 6.531\n",
       "1092043                -1.344\n",
       "1092044                -4.094\n",
       "1092045                 3.375\n",
       "1092046                 6.375\n",
       "1092047                 5.312\n",
       "1092048                 4.906\n",
       "1092049                -4.969\n",
       "1092050                -2.406\n",
       "1092051                -7.125\n",
       "1092052                 3.281\n",
       "1092053                -6.844\n",
       "1092054                     9\n",
       "1092055                 3.156\n",
       "1092056                -1.594\n",
       "1092057                     2\n",
       "1092058                 8.906\n",
       "1092059                -3.344\n",
       "Name: 3, Length: 1092059, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 873647 samples, validate on 218412 samples\n",
      "Epoch 1/1\n",
      "873600/873647 [============================>.] - ETA: 0s - loss: 0.7802 - mean_squared_error: 26.6783Epoch 00001: val_loss improved from inf to 0.76511, saving model to min_vl_model.h5\n",
      "873647/873647 [==============================] - 844s 966us/step - loss: 0.7802 - mean_squared_error: 26.6784 - val_loss: 0.7651 - val_mean_squared_error: 26.3523\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "#Adding the input layer and first hidden layer\n",
    "model.add(Dense(units =480, kernel_initializer='random_uniform', activation= 'tanh', \n",
    "                input_dim=train_x_arr.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "#Add the second hidden layer\n",
    "model.add(Dense(units =240, kernel_initializer='random_uniform', activation= 'tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "#Add the second hidden layer\n",
    "\n",
    "model.add(Dense(units =120, kernel_initializer='random_uniform', activation= 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units =60, kernel_initializer='random_uniform', activation= 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units =10, kernel_initializer='random_uniform', activation= 'relu'))\n",
    "#The output layer\n",
    "model.add(Dense(units =1, kernel_initializer='random_uniform', activation= 'elu'))\n",
    "\n",
    "#Compiling the ANN\n",
    "opt = keras.optimizers.Adam(lr=0.0015)\n",
    "model.compile(optimizer=opt, loss='mean_squared_logarithmic_error', metrics=['mse'])\n",
    "#Fitting the ANN to the training set\n",
    "model_filepath = 'min_vl_model.h5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor = 'val_loss', verbose=1, save_best_only = True, mode='min' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 873647 samples, validate on 218412 samples\n",
      "Epoch 1/2\n",
      "    96/873647 [..............................] - ETA: 25:23 - loss: 0.8057 - mean_squared_error: 28.3227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873632/873647 [============================>.] - ETA: 0s - loss: 0.7717 - mean_squared_error: 26.5325Epoch 00001: val_loss did not improve\n",
      "873647/873647 [==============================] - 929s 1ms/step - loss: 0.7717 - mean_squared_error: 26.5325 - val_loss: 0.7688 - val_mean_squared_error: 26.4161\n",
      "Epoch 2/2\n",
      "873632/873647 [============================>.] - ETA: 0s - loss: 0.7718 - mean_squared_error: 26.5375Epoch 00002: val_loss improved from 0.76511 to 0.76407, saving model to min_vl_model.h5\n",
      "873647/873647 [==============================] - 881s 1ms/step - loss: 0.7718 - mean_squared_error: 26.5374 - val_loss: 0.7641 - val_mean_squared_error: 26.2659\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_x_arr,train_y, validation_split=0.2, batch_size=32, nb_epoch=2, callbacks=[checkpoint])\n",
    "model.load_weights(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "test_csv = pd.read_csv('test.csv', delimiter= \",\", encoding='utf-8', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x1 = train_csv[1][1:]\n",
    "test_x2 = train_csv[2][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "for i in range(1,537881):\n",
    "#     print((train_x2[i]) )\n",
    "#     print( jokes.iloc[ int(train_x2[i]) ] )\n",
    "    row = np.concatenate(([int(test_x1[i])], np.transpose(np.array(jokes.iloc[ int(test_x2[i]) ]))) , axis= 0)\n",
    "#     print(row)\n",
    "    test_x.append(row)\n",
    "    \n",
    "# train_x\n",
    "# train_x1 = np.array(train_x)\n",
    "# train_x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_arr = np.array(test_x)\n",
    "y_pred1 = model.predict(test_x_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred[\"id\"] = test_csv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.rename(columns={0: \"Rating\"})\n",
    "y_pred = y_pred[[\"id\",\"Rating\"]]\n",
    "y_pred.to_csv(\"Submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(537880, 1)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(537881, 3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate((y_pred1, y_pred2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output.csv\", \"w\") \n",
    "f.write(\"id,Rating\\n\")\n",
    "for i in range(537880):\n",
    "    row = str(test_csv[0][i+1]) + \",\" + str(float(y_pred1[i])) + \"\\n\"\n",
    "    f.write(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>user_id</td>\n",
       "      <td>joke_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6194_11</td>\n",
       "      <td>6194</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19356_3</td>\n",
       "      <td>19356</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23426_79</td>\n",
       "      <td>23426</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40030_3</td>\n",
       "      <td>40030</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19806_115</td>\n",
       "      <td>19806</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10487_45</td>\n",
       "      <td>10487</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19959_8</td>\n",
       "      <td>19959</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10979_66</td>\n",
       "      <td>10979</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34886_6</td>\n",
       "      <td>34886</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4729_103</td>\n",
       "      <td>4729</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>29445_94</td>\n",
       "      <td>29445</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5071_22</td>\n",
       "      <td>5071</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17619_87</td>\n",
       "      <td>17619</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35698_59</td>\n",
       "      <td>35698</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24721_125</td>\n",
       "      <td>24721</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13032_7</td>\n",
       "      <td>13032</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3766_23</td>\n",
       "      <td>3766</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6287_137</td>\n",
       "      <td>6287</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1876_87</td>\n",
       "      <td>1876</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16375_3</td>\n",
       "      <td>16375</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17777_40</td>\n",
       "      <td>17777</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>27966_79</td>\n",
       "      <td>27966</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>39276_9</td>\n",
       "      <td>39276</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20382_6</td>\n",
       "      <td>20382</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>23184_59</td>\n",
       "      <td>23184</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>31703_12</td>\n",
       "      <td>31703</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9642_75</td>\n",
       "      <td>9642</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24627_68</td>\n",
       "      <td>24627</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1215_5</td>\n",
       "      <td>1215</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537851</th>\n",
       "      <td>38107_83</td>\n",
       "      <td>38107</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537852</th>\n",
       "      <td>6885_40</td>\n",
       "      <td>6885</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537853</th>\n",
       "      <td>33502_111</td>\n",
       "      <td>33502</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537854</th>\n",
       "      <td>16015_2</td>\n",
       "      <td>16015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537855</th>\n",
       "      <td>27776_40</td>\n",
       "      <td>27776</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537856</th>\n",
       "      <td>21378_68</td>\n",
       "      <td>21378</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537857</th>\n",
       "      <td>32531_83</td>\n",
       "      <td>32531</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537858</th>\n",
       "      <td>13965_44</td>\n",
       "      <td>13965</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537859</th>\n",
       "      <td>20301_6</td>\n",
       "      <td>20301</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537860</th>\n",
       "      <td>33943_76</td>\n",
       "      <td>33943</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537861</th>\n",
       "      <td>17448_56</td>\n",
       "      <td>17448</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537862</th>\n",
       "      <td>32704_7</td>\n",
       "      <td>32704</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537863</th>\n",
       "      <td>2012_100</td>\n",
       "      <td>2012</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537864</th>\n",
       "      <td>30931_53</td>\n",
       "      <td>30931</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537865</th>\n",
       "      <td>15990_22</td>\n",
       "      <td>15990</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537866</th>\n",
       "      <td>10937_25</td>\n",
       "      <td>10937</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537867</th>\n",
       "      <td>8773_115</td>\n",
       "      <td>8773</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537868</th>\n",
       "      <td>12221_13</td>\n",
       "      <td>12221</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537869</th>\n",
       "      <td>31653_114</td>\n",
       "      <td>31653</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537870</th>\n",
       "      <td>6556_55</td>\n",
       "      <td>6556</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537871</th>\n",
       "      <td>12196_88</td>\n",
       "      <td>12196</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537872</th>\n",
       "      <td>26145_7</td>\n",
       "      <td>26145</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537873</th>\n",
       "      <td>23510_43</td>\n",
       "      <td>23510</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537874</th>\n",
       "      <td>39267_2</td>\n",
       "      <td>39267</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537875</th>\n",
       "      <td>1857_5</td>\n",
       "      <td>1857</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537876</th>\n",
       "      <td>14333_74</td>\n",
       "      <td>14333</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537877</th>\n",
       "      <td>25245_16</td>\n",
       "      <td>25245</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537878</th>\n",
       "      <td>4082_9</td>\n",
       "      <td>4082</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537879</th>\n",
       "      <td>19638_128</td>\n",
       "      <td>19638</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537880</th>\n",
       "      <td>40665_26</td>\n",
       "      <td>40665</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537881 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0        1        2\n",
       "0              id  user_id  joke_id\n",
       "1         6194_11     6194       11\n",
       "2         19356_3    19356        3\n",
       "3        23426_79    23426       79\n",
       "4         40030_3    40030        3\n",
       "5       19806_115    19806      115\n",
       "6        10487_45    10487       45\n",
       "7         19959_8    19959        8\n",
       "8        10979_66    10979       66\n",
       "9         34886_6    34886        6\n",
       "10       4729_103     4729      103\n",
       "11       29445_94    29445       94\n",
       "12        5071_22     5071       22\n",
       "13       17619_87    17619       87\n",
       "14       35698_59    35698       59\n",
       "15      24721_125    24721      125\n",
       "16        13032_7    13032        7\n",
       "17        3766_23     3766       23\n",
       "18       6287_137     6287      137\n",
       "19        1876_87     1876       87\n",
       "20        16375_3    16375        3\n",
       "21       17777_40    17777       40\n",
       "22       27966_79    27966       79\n",
       "23        39276_9    39276        9\n",
       "24        20382_6    20382        6\n",
       "25       23184_59    23184       59\n",
       "26       31703_12    31703       12\n",
       "27        9642_75     9642       75\n",
       "28       24627_68    24627       68\n",
       "29         1215_5     1215        5\n",
       "...           ...      ...      ...\n",
       "537851   38107_83    38107       83\n",
       "537852    6885_40     6885       40\n",
       "537853  33502_111    33502      111\n",
       "537854    16015_2    16015        2\n",
       "537855   27776_40    27776       40\n",
       "537856   21378_68    21378       68\n",
       "537857   32531_83    32531       83\n",
       "537858   13965_44    13965       44\n",
       "537859    20301_6    20301        6\n",
       "537860   33943_76    33943       76\n",
       "537861   17448_56    17448       56\n",
       "537862    32704_7    32704        7\n",
       "537863   2012_100     2012      100\n",
       "537864   30931_53    30931       53\n",
       "537865   15990_22    15990       22\n",
       "537866   10937_25    10937       25\n",
       "537867   8773_115     8773      115\n",
       "537868   12221_13    12221       13\n",
       "537869  31653_114    31653      114\n",
       "537870    6556_55     6556       55\n",
       "537871   12196_88    12196       88\n",
       "537872    26145_7    26145        7\n",
       "537873   23510_43    23510       43\n",
       "537874    39267_2    39267        2\n",
       "537875     1857_5     1857        5\n",
       "537876   14333_74    14333       74\n",
       "537877   25245_16    25245       16\n",
       "537878     4082_9     4082        9\n",
       "537879  19638_128    19638      128\n",
       "537880   40665_26    40665       26\n",
       "\n",
       "[537881 rows x 3 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manikant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stpwords = stopwords.words('english') + list(string.punctuation); print(stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes['words'] = [sentence.split(\" \") for sentence in jokes[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes['sentences'] = [\" \".join([w for w in query if w not in stpwords]) for query in jokes['words'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes[\"sentences\"] = [\"\".join([w for w in query if w not in string.punctuation]) for query in jokes['sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b85033effe03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "import re\n",
    "words = re.compile(r\"\\w+\",re.I)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "queries = []\n",
    "for q in jokes['sentences'].tolist():\n",
    "        queries.append([stemmer.stem(i.lower()) for i in words.findall(q)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes['sentences'] = [\" \".join([w for w in sentence]) for sentence in queries]\n",
    "del queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               joketext\n",
       "1      q what o j simpson web address a slash slash b...\n",
       "2      how mani feminist take screw light bulb that f...\n",
       "3      q did hear dyslex devil worship a he sold soul...\n",
       "4      they ask japanes visitor elect countri everi m...\n",
       "5      q what blind person say given matzah a who hel...\n",
       "6              q what orang sound like parrot a a carrot\n",
       "7      how mani men take screw light bulb one men scr...\n",
       "8      a dog walk western union ask clerk send telegr...\n",
       "9      q if person speak three languag call trilingu ...\n",
       "10     what differ macintosh etchasketch you shake ma...\n",
       "11     what differ use tire 365 use condom one goodye...\n",
       "12     a duck walk pharmaci ask condom the pharmacist...\n",
       "13     q what australian word boomerang come back a a...\n",
       "14             what get run parakeet lawnmow shred tweet\n",
       "15     two kindergarten girl talk outsid one said you...\n",
       "16     a guy walk bar sit next extrem gorgeou woman t...\n",
       "17     bill clinton return vacat arkansa walk step ai...\n",
       "18     a mechan engin electr engin softwar engin micr...\n",
       "19     an old scotsman sit younger scottish gentleman...\n",
       "20     q what differ lawyer plumber a a plumber work ...\n",
       "21     presid clinton look desk oval offic see one ai...\n",
       "22     a man arriv gate heaven st peter ask religion ...\n",
       "23       what call american final world cup hey beer man\n",
       "24     out backwood midwestern state littl johnni arr...\n",
       "25     an explor deepest amazon suddenli find surroun...\n",
       "26     a guy walk bar order beer say bartend hey i go...\n",
       "27     a jewish young man see psychiatrist eat sleep ...\n",
       "28     may i take order waiter ask ye prepar chicken ...\n",
       "29     what differ men women a woman want one man sat...\n",
       "                             ...                        \n",
       "110    judi troubl comput call toni comput guy desk t...\n",
       "111    a drunk stagger cathol church enter confession...\n",
       "112    an astronom physicist mathematician it said ho...\n",
       "113    when peopl claim kill time express when chuck ...\n",
       "114    person 1 hey wanna hear great knockknock joke ...\n",
       "115    an american tourist goe restaur spain order sp...\n",
       "116    a briton frenchman russian view paint adam eve...\n",
       "117    a littl boy goe dad ask what polit hi dad say ...\n",
       "118    an american scot canadian terribl car accid th...\n",
       "119    a group girlfriend vacat see 5stori hotel sign...\n",
       "120    an old man goe doctor yearli physic wife tag a...\n",
       "121    a guy feel long final decid seek aid psychiatr...\n",
       "122    mickey mous nasti divorc minni mous mickey spo...\n",
       "123    the new employe stood paper shredder look conf...\n",
       "124    an artist ask galleri owner interest paint cur...\n",
       "125    a guy walk past mental hospit heard moan voic ...\n",
       "126    a man went appli job after fill applic wait an...\n",
       "127    deep within forest littl turtl began climb tre...\n",
       "128    washington reuter a tragic fire monday destroy...\n",
       "129    in veteran day speech presid bush vow we finis...\n",
       "130    chuck norri calendar goe straight march 31st a...\n",
       "131               jack bauer get mcdonald breakfast 1030\n",
       "132    one day three men went shrine ask father forgi...\n",
       "133    a preist 12yearold kid smartest guy world plan...\n",
       "134    a man drive countri one even car stall start h...\n",
       "135    a blond brunett red head line shot death fire ...\n",
       "136    america 800 welcom work 1200 lunch break 1700 ...\n",
       "137    it day big sale rumor sale and advertis local ...\n",
       "138    recent teacher garbag collector lawyer wound t...\n",
       "139    a littl girl ask father daddi do fairi tale be...\n",
       "Name: sentences, Length: 140, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 3000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "#should change the file for speed putting small one\n",
    "f = codecs.open('/Users/manikant/Documents/kaggle_challenges/quora_pairs/input/glove.6B.50d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(jokes['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_1 = tokenizer.texts_to_sequences(jokes['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sequences_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Null word embeddings: 388\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 3000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.array(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"data_1.csv\", data_1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
